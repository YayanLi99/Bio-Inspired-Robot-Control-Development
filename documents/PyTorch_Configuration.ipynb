{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ab6d960",
   "metadata": {},
   "source": [
    "# The PyTorch Configuration file\n",
    "\n",
    "The `pytorch_config.yaml` file specifies the configuration for your PyTorch pose models,\n",
    "from the model architecture to which optimizer will be used for training, how training \n",
    "runs will be logged, the data augmentation that will be applied and which metric should\n",
    "be used to save the \"best\" model snapshot. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1044e3",
   "metadata": {},
   "source": [
    "## Singleton Parameters\n",
    "\n",
    "\n",
    "- `device`: The device to use for training/inference. The default is `auto`, which sets the device to `cuda` if an NVIDIA GPU is available, and `cpu` otherwise. \n",
    "- `method`: Either `bu` for bottom-up models, or `td` for top-down models.\n",
    "- `net_type`: The type of pose model configured by the file (e.g. `resnet_50`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba82835d",
   "metadata": {},
   "source": [
    "\n",
    "## Data\n",
    "\n",
    "\n",
    "- `inference`: which transformations should be applied to images when running evaluation or inference\n",
    "- `train`: which transformations should be applied to images when training\n",
    "\n",
    "The default configuration for a pose model is:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d59fbb",
   "metadata": {
    "vscode": {
     "languageId": "xml"
    }
   },
   "outputs": [],
   "source": [
    "data:\n",
    "  bbox_margin: 20\n",
    "  colormode: RGB  # should never be changed\n",
    "  inference:  # the augmentations to apply to images during inference \n",
    "    normalize_images: true  # this should always be set to true\n",
    "  train:\n",
    "    affine:\n",
    "      p: 0.5                     # float: the probability that an affine transform is applied\n",
    "      rotation: 30               # int: the maximum angle of rotation applied to the image (in degrees)\n",
    "      scaling: [0.5, 1.25]       # [float, float]: the (min, max) scale to use to resize images\n",
    "      translation: 0             # int: the maximum translation to apply to images (in pixels)\n",
    "    covering: true\n",
    "    crop_sampling:\n",
    "      width: 448   # if your images are very small or very large, you may need to edit!\n",
    "      height: 448  # see below for more information about crop_sampling! \n",
    "      max_shift: 0.1\n",
    "      method: hybrid\n",
    "    gaussian_noise: 12.75\n",
    "    motion_blur: true\n",
    "    normalize_images: true  # this should always be set to true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2878d4a1",
   "metadata": {},
   "source": [
    "\n",
    "```yaml\n",
    "affine:\n",
    "  p: 0.9  # float: the probability that an affine transform is applied\n",
    "  rotation: 30  # int: the maximum angle of rotation applied to the image (in degrees)\n",
    "  scaling: [ 0.5, 1.25 ]  # [float, float]: the (min, max) scale to use to resize images\n",
    "  translation: 40  # int: the maximum translation to apply to images (in pixels)\n",
    "```\n",
    "\n",
    "**Auto-Padding**: Pads the image to some desired shape (e.g., a minimum height/width or \n",
    "such that the height/width are divisible by a given number). Some backbones (such as\n",
    "HRNets) require the height and width of images to be multiples of 32. Setting up\n",
    "auto-padding with `pad_height_divisor: 32` and `pad_width_divisor: 32` ensures that is\n",
    "the case. Note that **not all keys need to be set**! The values shown are the default\n",
    "values. Only one of 'min_height' and 'pad_height_divisor' parameters must be set, and \n",
    "only one of 'min_width' and 'pad_width_divisor' parameters must be set.\n",
    "\n",
    "```yaml\n",
    "auto_padding:\n",
    "  min_height: null  # int: if not None, the minimum height of the image\n",
    "  min_width: null  # int: if not None, the minimum width of the image\n",
    "  pad_height_divisor: null  # int: if not None, ensures image height is dividable by value of this argument.\n",
    "  pad_width_divisor: null  # int: if not None, ensures image width is dividable by value of this argument.\n",
    "  position: random  # str: position of the image, one of 'A.PadIfNeeded.Position'\n",
    "  border_mode: reflect_101  # str: 'constant' or 'reflect_101' (see cv2.BORDER modes)\n",
    "  border_value: null  # str: padding value if border_mode is 'constant'\n",
    "  border_mask_value: null  # str: padding value for mask if border_mode is 'constant'\n",
    "```\n",
    "\n",
    "**Covering**: Based on Albumentations's [CoarseDropout](\n",
    "https://albumentations.ai/docs/api_reference/augmentations/dropout/coarse_dropout/#albumentations.augmentations.dropout.coarse_dropout)\n",
    "augmentation, this \"cuts\" holes out of the image. As defined in \n",
    "[Improved Regularization of Convolutional Neural Networks with Cutout](\n",
    "https://arxiv.org/abs/1708.04552).\n",
    "\n",
    "```yaml\n",
    "covering: true  # bool: if true, applies a coarse dropout with probability 50%\n",
    "```\n",
    "\n",
    "**Gaussian Noise**: Applies gaussian noise to the input image. Can either be a float \n",
    "(the standard deviation of the noise) or simply a boolean (the standard deviation of \n",
    "the noise will be set as 12.75).\n",
    "\n",
    "```yaml\n",
    "gaussian_noise: 12.75  # bool, float: add gaussian noise\n",
    "```\n",
    "\n",
    "**Horizontal Flips**: This flips the image horizontally around the y-axis. As the \n",
    "resulting image is mirrored, it does not preserve labels (the left hand would become the\n",
    "right hand, and vice versa). This augmentation should not be used for pose models if you\n",
    "have symmetric keypoints! However, it is safe to use it to train detectors. If you want\n",
    "to use horizontal flips with symmetric keypoints, you need to specify them through the \n",
    "`symmetries` parameter!\n",
    "\n",
    "```yaml\n",
    "# augmentation for object detectors or when no symmetric (left/right) keypoints exist: \n",
    "hflip: true\n",
    "\n",
    "# augmentation if your bodyparts are [snout, eye_L, eye_R, ear_L, ear_R]\n",
    "hflip:\n",
    "  p: 0.5  # apply a horizontal flip with 50% probability\n",
    "  symmetries: [[1, 2], [3, 4]]  # the indices of symmetric keypoints\n",
    "```\n",
    "\n",
    "**Histogram Equalization**: Applies histogram equalization with probability 50%.\n",
    "\n",
    "```yaml\n",
    "hist_eq: true  # bool: whether to apply histogram equalization\n",
    "```\n",
    "\n",
    "**Motion Blur**: Applies motion blur to the image with probability 50%.\n",
    "\n",
    "```yaml\n",
    "motion_blur: true  # bool: whether to apply motion blur\n",
    "```\n",
    "\n",
    "**Normalization**: This should always be set to `true`.\n",
    "\n",
    "```yaml\n",
    "normalize_images: true  # normalizes images\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe51fc8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Dealing with Variable Image Sizes\n",
    "\n",
    "```{NOTE}\n",
    "When training with batch size 1 (or if all images in your dataset have the same size), \n",
    "you don't need to worry about any of this! However, you can still use `crop_sampling`\n",
    "which may help your model generalize.\n",
    "```\n",
    "\n",
    "When training with a batch size greater than 1, all images in a batch **must** have the\n",
    "same size. PyTorch **collates** all images into one tensor of shape `[b, c, h, w]`, \n",
    "where `b` is the batch size, `c` the number of channels in the image, `h` and `w` the \n",
    "height and width of images in the batches. There are a few different ways to ensure that\n",
    "all images in a batch have the same size:\n",
    "\n",
    "1. **Crop sampling**. This is the default behavior for the PyTorch engine in DeepLabCut.\n",
    "A part of each image (of a fixed size) is cropped and given to the model to train. See \n",
    "below for more information.\n",
    "2. **A custom collate function**. Collate functions define a way that images of different\n",
    "sizes can be combined into one tensor. This involves resizing and padding images to the\n",
    "same size and aspect ratio. Available collate functions are defined in\n",
    "`deeplabcut/pose_estimation_pytorch/data/collate.py`. \n",
    "3. **Resizing all images**. All images can simply be resized to the same size. This\n",
    "usually doesn't lead to the best performance.\n",
    "\n",
    "**Resizing - Crop Sampling**: An alternative way to ensure all images have the same size\n",
    "is through cropping. The `crop_sampling` crops images down to a maximum width and \n",
    "height, with options to sample the center of the crop according to the positions of the\n",
    "keypoints. The methods to sample the center of the crop are as follows:\n",
    "\n",
    "- `uniform`: randomly over the image\n",
    "- `keypoints`: randomly over the annotated keypoints\n",
    "- `density`: weighing preferentially dense regions of keypoints\n",
    "- `hybrid`: alternating randomly between `uniform` and `density`\n",
    "\n",
    "```yaml\n",
    "crop_sampling:\n",
    "  height: 400  # int: the height of the crop \n",
    "  width: 400  # int: the height of the crop \n",
    "  max_shift: 0.4  # float: maximum allowed shift of the cropping center position as a fraction of the crop size.\n",
    "  method: hybrid # str: the center sampling method (one of 'uniform', 'keypoints', 'density', 'hybrid') \n",
    "```\n",
    "\n",
    "**Collate**: Defines how images are collated into batches. The default way collate\n",
    "function to use is `ResizeFromDataSizeCollate` (other collate functions are defined in\n",
    "`deeplabcut/pose_estimation_pytorch/data/collate.py`). For each batch to collate, this\n",
    "implementation:\n",
    "1. Selects the target width & height all images will be resized to by getting the size \n",
    "of the first image in the batch, and multiplying it by a scale sampled uniformly at \n",
    "random from `(min_scale, max_scale)`.\n",
    "2. Resizes all images in the batch (while preserving their aspect ratio) such that they \n",
    "are the smallest size such that the target size fits entirely in the image.\n",
    "3. Crops each resulting image into the target size with a random crop.\n",
    "\n",
    "```yaml\n",
    "collate:  # rescales the images when putting them in a batch\n",
    "  type: ResizeFromDataSizeCollate  # You can also use `ResizeFromListCollate`\n",
    "  max_shift: 10  # the maximum shift, in pixels, to add to the random crop (this means\n",
    "    # there can be a slight border around the image)\n",
    "  max_size: 1024  #  the maximum size of the long edge of the image when resized. If the\n",
    "    # longest side will be greater than this value, resizes such that the longest side \n",
    "    # is this size, and the shortest side is smaller than the desired size. This is \n",
    "    # useful to keep some information from images with extreme aspect ratios.\n",
    "  min_scale: 0.4  # the minimum scale to resize the image with\n",
    "  max_scale: 1.0  # the maximum scale to resize the image with\n",
    "  min_short_side: 128  # the minimum size of the target short side\n",
    "  max_short_side: 1152  # the maximum size of the target short side\n",
    "  multiple_of: 32  # pads the target height, width such that they are multiples of 32\n",
    "  to_square: false  # instead of using the aspect ratio of the first image, only the \n",
    "    # short side of the first image will be used to sample a \"side\", and the images will\n",
    "    # be cropped in squares\n",
    "```\n",
    "\n",
    "**Resizing**: Resizes the images while preserving the aspect ratio (first resizes to the\n",
    "maximum possible size, then adds padding for the missing pixels).\n",
    "\n",
    "```yaml\n",
    "resize:\n",
    "  height: 640 # int: the height to which all images will be resized\n",
    "  width: 480 # int: the width to which all images will be resized\n",
    "  keep_ratio: true  # bool: whether the aspect ratio should be preserved when resizing\n",
    "```\n",
    "\n",
    "### Model\n",
    "\n",
    "The model configuration is further split into a `backbone`, optionally a `neck` and a \n",
    "number of heads.\n",
    "\n",
    "Changing the `model` configuration should only be done by expert users, and in rare \n",
    "occasions. When updating a model configuration (e.g. adding more deconvolution layers \n",
    "to a `HeatmapHead`) must be done in a way where the model configuration still makes \n",
    "sense for the project (e.g. the number of heatmaps output needs to match the number of \n",
    "bodyparts in the project).\n",
    "\n",
    "An example model configuration for a single-animal HRNet would look something like:\n",
    "\n",
    "```yaml\n",
    "model:\n",
    "  backbone:  # the BaseBackbone used by the pose model\n",
    "    type: HRNet\n",
    "    model_name: hrnet_w18  # creates an HRNet W18 backbone\n",
    "  backbone_output_channels: 18\n",
    "  heads:  # configures how the different heads will make predictions\n",
    "    bodypart:  # configures how pose will be predicted for bodyparts\n",
    "      type: HeatmapHead\n",
    "      predictor:  # the BasePredictor used to make predictions from the head's outputs\n",
    "        type: HeatmapPredictor\n",
    "          ...\n",
    "      target_generator:  # the BaseTargetGenerator used to create targets for the head\n",
    "        type: HeatmapPlateauGenerator\n",
    "          ...\n",
    "      criterion:  # the loss criterion used for the head\n",
    "        ...\n",
    "      ...  # head-specific options, such as `heatmap_config` or `locref_config` for a \"HeatmapHead\"\n",
    "```\n",
    "\n",
    "The `backbone`, `neck` and `head` configurations are loaded using the\n",
    "`deeplabcut.pose_estimation_pytorch.models.backbones.base.BACKBONES`,\n",
    "`deeplabcut.pose_estimation_pytorch.models.necks.base.NECKS` and \n",
    "`deeplabcut.pose_estimation_pytorch.models.heads.base.HEADS` registries. You specify \n",
    "which type to load with the `type` parameter. Any argument for the head can then be used\n",
    "in the configuration.\n",
    "\n",
    "So to use an `HRNet` backbone for your model (as defined in \n",
    "`deeplabcut.pose_estimation_pytorch.models.backbones.hrnet.HRNet`), you could set:\n",
    "\n",
    "```yaml\n",
    "model:\n",
    "  backbone:\n",
    "    type: HRNet\n",
    "    model_name: hrnet_w32  # creates an HRNet W32\n",
    "    pretrained: true  # the backbone weights for training will be loaded from TIMM (pre-trained on ImageNet)\n",
    "    interpolate_branches: false  # don't interpolate & concatenate channels from all branches \n",
    "    increased_channel_count: true  # use the incre_modules defined in the TIMM HRNet\n",
    "  backbone_output_channels: 128  # number of channels output by the backbone\n",
    "```\n",
    "\n",
    "### Runner\n",
    "\n",
    "The runner contains elements relating to the runner to use (including the optimizer and \n",
    "learning rate schedulers). Unless you're experienced with machine learning and training \n",
    "models **it is not recommended to change the optimizer or scheduler**.\n",
    "\n",
    "```yaml\n",
    "runner:\n",
    "  type: PoseTrainingRunner  # should not need to modify this\n",
    "  key_metric: \"test.mAP\"  # the metric to use to select the \"best snapshot\"\n",
    "  key_metric_asc: true  # whether \"larger=better\" for the key_metric\n",
    "  eval_interval: 1  # the interval between each passes through the evaluation dataset\n",
    "  optimizer:  # the optimizer to use to train the model\n",
    "    ...\n",
    "  scheduler:  # optional: a learning rate scheduler\n",
    "    ...\n",
    "  load_scheduler_state_dict: true/false # whether to load scheduler state when resuming training from a snapshot,\n",
    "  snapshots:  # parameters for the TorchSnapshotManager\n",
    "    max_snapshots: 5  # the maximum number of snapshots to save (the \"best\" model does not count as one of them)\n",
    "    save_epochs: 25  # the interval between each snapshot save  \n",
    "    save_optimizer_state: false  # whether the optimizer state should be saved with the model snapshots (very little reason to set to true)\n",
    "  gpus: # GPUs to use to train the network\n",
    "  - 0\n",
    "  - 1\n",
    "```\n",
    "\n",
    "**Key metric**: Every time the model is evaluated on the test set, metrics are computed \n",
    "to see how the model is performing. The key metric is used to determine whether the \n",
    "current model is the \"best\" so far. If it is, the snapshot is saved as `...-best.pt`. \n",
    "For pose models, metrics to choose from would be `test.mAP` (with `key_metric_asc: true`\n",
    ") or `test.rmse` (with `key_metric_asc: false`). \n",
    "\n",
    "**Evaluation interval**: Evaluation slows down training (it takes time to go through all\n",
    "the evaluation images, make predictions and log results!). So instead of evaluating \n",
    "after every epoch, you could decide to evaluate every 5 epochs (by setting\n",
    "`eval_interval: 5`). While this means you get coarser information about how your model \n",
    "is training, it can speed up training on large datasets.\n",
    "\n",
    "**Optimizer**: Any optimizer inheriting `torch.optim.Optimizer`. More information about \n",
    "optimizers can be found in [PyTorch's documentation](\n",
    "https://pytorch.org/docs/stable/optim.html). Examples:\n",
    "\n",
    "```yaml\n",
    "  # SGD with initial learning rate 1e-3 and momentum 0.9\n",
    "  #  see https://pytorch.org/docs/stable/generated/torch.optim.SGD.html\n",
    "  optimizer:\n",
    "    type: SGD\n",
    "    params:\n",
    "      lr: 1e-3\n",
    "      momentum: 0.9\n",
    "\n",
    "  # AdamW optimizer with initial learning rate 1e-4\n",
    "  #  see https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html\n",
    "  optimizer:\n",
    "    type: AdamW\n",
    "    params:\n",
    "      lr: 1e-4\n",
    "```\n",
    "\n",
    "**Scheduler**: You can use [any scheduler](\n",
    "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate) defined in\n",
    "`torch.optim.lr_scheduler`, where the arguments given are arguments of the scheduler. \n",
    "The default scheduler is an LRListScheduler, which changes the learning rates at each \n",
    "milestone to the corresponding values in `lr_list`. Examples:\n",
    "\n",
    "```yaml\n",
    "  # reduce to 1e-5 at epoch 160 and 1e-6 at epoch 190\n",
    "  scheduler:\n",
    "    type: LRListScheduler\n",
    "    params:\n",
    "      lr_list: [ [ 1e-5 ], [ 1e-6 ] ]\n",
    "      milestones: [ 160, 190 ]\n",
    "\n",
    "  # Decays the learning rate of each parameter group by gamma every step_size epochs\n",
    "  #   see https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html\n",
    "  scheduler:\n",
    "    type: StepLR\n",
    "    params:\n",
    "      step_size: 100\n",
    "      gamma: 0.1\n",
    "```\n",
    "\n",
    "You can also use schedulers that use other schedulers as parameters, such as a \n",
    "[`ChainedScheduler`](\n",
    "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ChainedScheduler.html)\n",
    "or a [`SequentialLR`](\n",
    "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.SequentialLR.html).\n",
    "\n",
    "The `SequentialLR` can be particularly useful, such as to use a first scheduler for some\n",
    "warmup epochs, and a second scheduler later. An example usage would be:\n",
    "\n",
    "```yaml\n",
    "  # Multiply the learning rate by `factor` for the first `total_iters` epochs\n",
    "  # After 5 epochs, start decaying the learning rate by `gamma` every `step_size` epochs\n",
    "  # If the initial learning rate is set to 1, the learning rates will be:\n",
    "  #   epoch 0: 0.01  - using ConstantLR\n",
    "  #   epoch 1: 0.01  - using ConstantLR\n",
    "  #   epoch 2: 1.0   - using ConstantLR\n",
    "  #   epoch 3: 1.0   - using ConstantLR\n",
    "  #   epoch 4: 1.0   - using ConstantLR\n",
    "  #   epoch 5: 1.0   - using StepLR\n",
    "  #   epoch 6: 1.0   - using StepLR\n",
    "  #   epoch 7: 0.1   - using StepLR\n",
    "  #   epoch 8: 0.1   - using StepLR\n",
    "  scheduler:\n",
    "    type: SequentialLR\n",
    "    params:\n",
    "      schedulers:\n",
    "      - type: ConstantLR\n",
    "        params:\n",
    "          factor: 0.01\n",
    "          total_iters: 2\n",
    "      - type: StepLR\n",
    "        params:\n",
    "          step_size: 2\n",
    "          gamma: 0.1\n",
    "      milestones:\n",
    "      - 5\n",
    "```\n",
    "\n",
    "### Train Settings\n",
    "\n",
    "The `train_settings` key contains parameters that are specific to training. For more \n",
    "information about the `dataloader_workers` and `dataloader_pin_memory` settings, see\n",
    "[Single- and Multi-process Data Loading](\n",
    "https://pytorch.org/docs/stable/data.html#single-and-multi-process-data-loading)\n",
    "and [memory pinning](https://pytorch.org/docs/stable/data.html#memory-pinning). Setting\n",
    "`dataloader_workers: 0` uses single-process data loading, while setting it to 1 or more\n",
    "will use multi-process data loading. You should always keep \n",
    "`dataloader_pin_memory: true` when training on an NVIDIA GPU. \n",
    "\n",
    "```yaml\n",
    "train_settings:\n",
    "  batch_size: 1  # the batch size used for training\n",
    "  dataloader_workers: 0  # the number of workers for the PyTorch Dataloader \n",
    "  dataloader_pin_memory: true  # pin DataLoader memory\n",
    "  display_iters: 500  # the number of iterations (steps) between each log print\n",
    "  epochs: 200  # the maximum number of epochs for which to train the model\n",
    "  seed: 42  # the random seed to set for reproducibility\n",
    "```\n",
    "\n",
    "### Logger\n",
    "\n",
    "Training runs are logged to the model folder (where the snapshots are stored) by \n",
    "default.\n",
    "\n",
    "Additionally, you can log results to [Weights and Biases](https://wandb.ai/site), by adding a\n",
    "`WandbLogger`. Just make sure you're logged in to your `wandb` account before starting \n",
    "your training run (with `wandb login` from your shell). For more information, see their\n",
    "[tutorials](https://docs.wandb.ai/tutorials) and their documentation for [`wandb.init`](https://docs.wandb.ai/ref/python/init).\n",
    "\n",
    "Logging to `wandb` is a good way to keep track of what you've run, including performance\n",
    "and metrics.\n",
    "\n",
    "```yaml\n",
    "logger:\n",
    " type: WandbLogger\n",
    " project_name: my-dlc3-project  # the name of the project where the run should be logged\n",
    " run_name: dekr-w32-shuffle0  # the name of the run to log\n",
    " ...  # any other argument you can pass to `wandb.init`, such as `tags: [\"dekr\", \"split=0\"]`\n",
    "```\n",
    "\n",
    "You can also log images as they are seen by the model to `wandb` \n",
    "with the `image_log_interval`. This logs a random train and test image, as well as the \n",
    "targets and heatmaps for that image.\n",
    "\n",
    "### Restarting Training at a Specific Checkpoint\n",
    "\n",
    "If you wish to restart the training at a specific checkpoint, you can specify the\n",
    "full path of the checkpoint to the `resume_training_from` variable, as shown below. In this \n",
    "example, `snapshot-010.pt` will be loaded before training starts, and the model will \n",
    "continue to train from the 10th epoch on.\n",
    "\n",
    "```yaml\n",
    "# model configuration\n",
    "...\n",
    "# weights from which to resume training\n",
    "resume_training_from: /Users/john/dlc-project-2021-06-22/dlc-models-pytorch/iteration-0/dlcJun22-trainset95shuffle0/train/snapshot-010.pt\n",
    "```\n",
    "\n",
    "When continuing to train a model, you may want to modify the learning rate scheduling \n",
    "that was being used (by editing the configuration under the `scheduler` key). When doing\n",
    "so, you *must set `load_scheduler_state_dict: false`* in your `runner` config! \n",
    "Otherwise, the parameters for the scheduler your started training with will be loaded \n",
    "from the state dictionary, and your edits might not be kept!\n",
    "\n",
    "## Training Top-Down Models\n",
    "\n",
    "Top-down models are split into two main elements: a detector (localizing individuals in\n",
    "the images) and a pose model predicting each individual's pose (once localization is \n",
    "done, obtaining pose is just like getting pose in a single-animal model!).\n",
    "\n",
    "The \"pose\" part of the model configuration is exactly the same as for single-animal or\n",
    "bottom-up models (configured through the `data`, `model`, `runner` and `train_settings`\n",
    "). The detector is configured through a detector key, at the top-level of the\n",
    "configuration.\n",
    "\n",
    "### Detector Configuration\n",
    "\n",
    "When training top-down models, you also need to configure how the detector will be \n",
    "trained. All information relating to the detector is placed under the `detector` key.\n",
    "\n",
    "```yaml\n",
    "detector:\n",
    "  data:  # which data augmentations will be used, same options as for the pose model\n",
    "    colormode: RGB\n",
    "    inference:  # default inference configuration for detectors\n",
    "      normalize_images: true\n",
    "    train:  # default train configuration for detectors\n",
    "      affine:\n",
    "        p: 0.9\n",
    "        rotation: 30\n",
    "        scaling: [ 0.5, 1.25 ]\n",
    "        translation: 40\n",
    "      hflip: true\n",
    "      normalize_images: true\n",
    "  model:  # the detector to train\n",
    "    type: FasterRCNN\n",
    "    variant: fasterrcnn_mobilenet_v3_large_fpn\n",
    "    pretrained: true\n",
    "  runner:  #  detector train runner configuration (same keys as for the pose model)\n",
    "    type: DetectorTrainingRunner\n",
    "    ...\n",
    "  train_settings: # detector train settings (same keys as for the pose model)\n",
    "    ...\n",
    "  resume_training_from: # optional: restart the training at the specific checkpoint\n",
    "```\n",
    "\n",
    "Currently, the only detectors available are `FasterRCNN` and `SSDLite`. However, multiple variants of\n",
    "`FasterRCNN` are available (you can view the different variants on \n",
    "[torchvision's object detection page](https://pytorch.org/vision/stable/models.html#object-detection)). It's recommended to use the fastest \n",
    "detector that brings enough performance. The recommended variants are the following \n",
    "(from fastest to most powerful, taken from torchvision's documentation):\n",
    "\n",
    "| name                              | Box MAP (larger = more powerful) | Params (larger = more powerful) | GFLOPS (larger = slower) |\n",
    "|-----------------------------------|---------------------------------:|--------------------------------:|-------------------------:|\n",
    "| SSDLite                           |                             21.3 |                            3.4M |                     0.58 |\n",
    "| fasterrcnn_mobilenet_v3_large_fpn |                             32.8 |                           19.4M |                     4.49 |\n",
    "| fasterrcnn_resnet50_fpn           |                               37 |                           41.8M |                   134.38 |\n",
    "| fasterrcnn_resnet50_fpn_v2        |                             46.7 |                           43.7M |                   280.37 |\n",
    "\n",
    "\n",
    "### Restarting Training of an Object Detector at a Specific Checkpoint\n",
    "\n",
    "If you wish to restart the training of a detector at a specific checkpoint, you can\n",
    "specify the full path of the checkpoint to the detector's `resume_training_from` variable, as\n",
    "shown below. In this example, `snapshot-detector-020.pt` will be loaded before training\n",
    "starts, and the model will continue to train from the 20th epoch on.\n",
    "\n",
    "```yaml\n",
    "detector:\n",
    "  # detector configuration\n",
    "  ...\n",
    "  # weights from which to resume training\n",
    "  resume_training_from: /Users/john/dlc-project-2021-06-22/dlc-models-pytorch/iteration-0/dlcJun22-trainset95shuffle0/train/snapshot-detector-020.pt\n",
    "```\n",
    "\n",
    "When continuing to train a detector, you may want to modify the learning rate scheduling \n",
    "that was being used (by editing the configuration under the `scheduler` key). When doing\n",
    "so, you *must set `load_scheduler_state_dict: false`* in your `detector`: `runner`\n",
    "config! Otherwise, the parameters for the scheduler your started training with will be\n",
    "loaded from the state dictionary, and your edits might not be kept!\n",
    "\n",
    "(bbox-from-pose)=\n",
    "### Generating Bounding Boxes from Pose\n",
    "\n",
    "To train object detection models (for top-down pose estimation), ground truth bounding\n",
    "boxes are needed. As they are not annotated in DeepLabCut, they are generated from the\n",
    "ground truth pose: simply take the minimum and maximum for the x and y axes, add a small\n",
    "margin and you have your bounding box! The default setting adds a margin of 20 pixels\n",
    "around the pose. This works well in most cases, but in some cases you should update this \n",
    "value (e.g. when you have very small or large images).\n",
    "\n",
    "You can edit that value in the `pytorch_config.yaml` for your model through the \n",
    "`data: bbox_margin` parameter for the detector:\n",
    "\n",
    "```yaml\n",
    "detector:\n",
    "  data:\n",
    "    bbox_margin: 20\n",
    "    ...\n",
    "```\n",
    "\n",
    "![Bounding boxes generated from pose with different margins](assets/bboxes_from_kpts.png)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
